{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"165px"},"toc_section_display":true,"toc_window_display":false},"colab":{"name":"week9/特徴量エンジニアリング.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"0j_tr8ArGAMT"},"source":["# 9 特徴量エンジニアリング"]},{"cell_type":"markdown","metadata":{"id":"bS6rSkkZGAMT"},"source":["今回は様々なデータセットを題材として特徴量エンジニアリングについて学びます。\n","\n","ゴール：特徴量エンジニアリングの手法を学び、適切に使い分けられるようになる。"]},{"cell_type":"markdown","metadata":{"id":"RPcqCqyiGAMT"},"source":["## 目次\n","**[9.1 はじめに](#9.1-はじめに)**  \n","> [9.1.1 準備](#9.1.1-準備)  \n","> [9.1.2 特徴量エンジニアリング概要](#9.1.2-特徴量エンジニアリング概要)  \n","\n","**[9.2 数値変数の変換](#9.2-数値変数の変換)**  \n","> [9.2.1 スケーリング](#9.2.1-スケーリング)  \n","> [9.2.2 非線形変換](#9.2.2-非線形変換)  \n","> [9.2.3 交差項の作成](#9.2.3-交差項の作成) \n","\n","**[9.3 カテゴリ変数のエンコーディング](#9.3-カテゴリ変数のエンコーディング)**  \n","> [9.3.1 Label Encoding](#9.3.1-Label-Encoding)  \n","> [9.3.2 Count Encoding](#9.3.2-Count-Encoding)  \n","> [9.3.3 Label-Count Encoding](#9.3.3-Label-Count-Encoding)  \n","> [9.3.4 One-Hot Encoding](#9.3.4-One-Hot-Encoding) \n","\n","**[9.4 時間変数のエンコーディング](#9.4-時間変数のエンコーディング)**  \n","> [9.4.1 周期性を考慮しないエンコーディング](#9.4.1-周期性を考慮しないエンコーディング)  \n","> [9.4.2 周期性を考慮したエンコーディング](#9.4.2-周期性を考慮したエンコーディング)  \n","\n","**[9.5 次元削減と特徴選択](#9.5-次元削減と特徴選択)**  \n","> [9.5.1 次元削減](#9.5.1-次元削減)  \n","> [9.5.2 特徴選択](#9.5.2-特徴選択)  "]},{"cell_type":"markdown","metadata":{"id":"o00BCfWCGAMT"},"source":["## 9.1 はじめに\n","### 9.1.1 準備\n","　まず基本的なライブラリの読み込みや、設定を済ませておきます。"]},{"cell_type":"code","metadata":{"id":"A2VO_XOAGAMT"},"source":["# ライブラリの読み込み\n","import numpy as np\n","import pandas as pd\n","from pandas import DataFrame, Series\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# その他設定\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ffqGav_5GAMU"},"source":["### 9.1.2 特徴量エンジニアリング概要\n","　特徴量エンジニアリングとは、モデルの学習・予測に効果的な説明変数を作成しようとする試みです。  \n","（説明変数と特徴量はほぼ同義ですが、前者は統計学・後者は機械学習分野での用語となっています。） \n"," \n","特徴量エンジニアリングの取り組みには、大別して二つの方向性があります;\n","- 問題設定に依らない汎用的な手法を用いるもの\n","- 問題設定に特有の背景知識に基づくもの\n","\n","今回は、汎用的な前者の手法の中から、代表的なものをいくつか紹介します。\n","\n","初めに、数値変数とカテゴリカル変数の違いについて確認しておきましょう。  \n","データの型には、大別して数値変数とカテゴリカル変数の二つがあります; \n","- 数値変数: 比例尺度・間隔尺度\n","- カテゴリカル変数: 順序尺度・名義尺度\n","\n","ほぼ全ての機械学習モデルの実装は、カテゴリカル変数を含むデータに対して学習や予測ができません。  \n","そのため、カテゴリカル変数は予め数値変数に変換しておく必要があります。（エンコーディング）"]},{"cell_type":"markdown","metadata":{"id":"Apckmg8_GAMU"},"source":["## 9.2 数値変数の変換\n","### 9.2.1 スケーリング\n","数値変数間のスケールが異なる場合、非決定木系のモデルでは適切に学習が行われないことがあります。   \n","そのような場合には、標準化や正規化と呼ばれる変換によって、数値変数に対しスケーリングを施します。\n","\n","- 標準化:   \n","　平均が0・標準偏差が1になるよう以下のように変換します。  \n","　ただし $\\mu_x$ と $\\sigma_x$ はそれぞれ $x$ の平均と標準偏差です。\n","\\begin{equation}\n","x_{std} = \\cfrac{x - \\mu_x}{\\sigma_x}\n","\\end{equation}\n","\n","- 正規化:  \n","　最小値が0・最大値が1になるよう以下のように変換します。\n","\\begin{equation}\n","x_{norm} = \\cfrac{x - x_{min}}{x_{max} - x_{min}}\n","\\end{equation}\n","\n","ここではアヤメデータセットを用いて、前者の標準化の適用方法とその使い分けについて見てみましょう。  \n","標準化は上の式に従って簡単に実装できますが、以下のように実装済みのものを利用することもできます。  \n","以下では、`sepal length (cm)` を sklearn.preprocessing.StandardScaler により標準化しています。"]},{"cell_type":"code","metadata":{"id":"BBMC9kG3GAMU"},"source":["# ライブラリの読み込み\n","from sklearn.datasets import load_iris\n","from sklearn.preprocessing import StandardScaler\n","\n","# データの読み込み\n","iris = load_iris()\n","iris = load_iris()\n","df = pd.DataFrame(iris.data, columns=iris.feature_names)\n","df['target'] = iris.target\n","\n","# 対象変数の取り出し\n","x = df[['sepal length (cm)']]\n","\n","# 標準化器の作成および適用\n","ss = StandardScaler()\n","x_std = ss.fit_transform(x)\n","\n","# 変換前と変換後の値を比較\n","x['sepal length std'] = x_std\n","display(x)\n","\n","# 変換前と変換後の平均と標準偏差を比較\n","x.describe().loc[['mean', 'std'], :]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vQv8Oqm3GAMU"},"source":["確かに標準化後は、平均が0・標準偏差が1となっています。\n","\n","アヤメデータセットで、モデルの種類およびスケーリングの有無に関する対照実験を行ってみましょう。\n","- 説明変数: `sepal length (cm)` (がくの長さ)\n","- 目的変数: `target` (アヤメの品種, 3クラス)\n","- モデル: ロジスティック回帰 / 決定木\n","- 変数変換: 標準化の有無\n","- 評価指標: 正解率\n","- 検証方法: ホールドアウト法"]},{"cell_type":"code","metadata":{"id":"-DMoIBDcGAMU"},"source":["# ライブラリの読み込み\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import accuracy_score\n","\n","# 説明変数と目的変数の用意\n","X = df[['sepal length (cm)']]\n","y = df.loc[:, \"target\"]\n","\n","# 訓練データと検証データの分割\n","X_train, X_valid, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# モデルの用意\n","mods = {\n","    'LogisticRegression': LogisticRegression(C=0.01, random_state=42),\n","\n","    'StandardScaler + LogisticRegression': Pipeline([('ss', StandardScaler()), ('model', LogisticRegression(C=0.01, random_state=42))]),\n","\n","    'DecisonTree': DecisionTreeClassifier(random_state=42),\n","\n","    'StandardScaler + DecisonTree': Pipeline([('ss', StandardScaler()), ('model', DecisionTreeClassifier(random_state=42))])\n","}\n","\n","# モデルの学習と評価\n","results = {}\n","for mod_name, mod in mods.items():\n","    mod.fit(X_train, y_train)\n","    results[(mod_name, 'train')] = round(accuracy_score(y_train, mod.predict(X_train)), 3)\n","    results[(mod_name, 'valid')] = round(accuracy_score(y_test, mod.predict(X_valid)), 3)\n","\n","# 結果の整理\n","pd.Series(results).unstack().iloc[[1, 3, 0, 2]]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1FXCRVwpGAMV"},"source":["この結果で押さえておくべき点は次の二つです;  \n","- 上の2行を比較すると、ロジスティック回帰では標準化した方が精度が高くなっています。\n","- 下の2行を比較すると、決定木では標準化の影響が全く現れていません。 \n","\n","このように標準化は、決定木系のモデルに影響を一切与えません。  \n","一方で非決定木系のモデルには良い影響を与えることがあります。"]},{"cell_type":"markdown","metadata":{"id":"jExb124OGAMV"},"source":["**<練習問題 1>**  \n","標準化はなぜ決定木系のモデルに影響を与えないのか考えて/調べてみましょう。  \n","（ヒント：決定木系のアルゴリズムでは、変数間のどのような関係が問題となるでしょうか？）"]},{"cell_type":"markdown","metadata":{"id":"KmWZHlr9GAMV"},"source":["**<練習問題 2>**  \n","今までに開催されたコンペティションのデータを用いて、数値変数のスケーリングの効果を検証してみましょう。"]},{"cell_type":"markdown","metadata":{"id":"kOqykCbyGAMV"},"source":["### 9.2.2 非線形変換\n","スケーリングで見たように、非決定木系のモデルでは数値変数を非線形変換した方が性能が向上することがあります。  \n","主な非線形変換としては対数・平方・平方根などがありますが、今回は対数変換を例にとってその効果を見てみます。\n","\n","対数変換は、その変数の分布が歪んでいる場合において、それを補正して分布を正規分布に近付けるために用います。  \n","歪んだ分布を対数変換によって正規分布に近付けることにより、機械学習モデルの性能が向上する場合があります。\n","\n","ここではワインデータセットの一部を用いて、対数変換の有無によるロジスティック回帰の性能の違いを見てみます。  \n","まずは、ワインデータセットの中から以下の二変数の分布と、それを対数変換したものの分布を比較してみましょう。\n","- color_intensity: 色の濃さ\n","- malic acid: リンゴ酸濃度\n","\n","ちなみに今回は問題とはなりませんが、対数変換を適用するためには変換前の変数が全て正値である必要があります。  \n","ゼロ以下の値を含んでいる場合には、最小値+1を加えるなどして変数を平行移動すれば、適用できるようになるます。"]},{"cell_type":"code","metadata":{"id":"t5bhmnkNGAMV"},"source":["# ライブラリの読み込み\n","from sklearn.datasets import load_wine\n","\n","# データの読み込み\n","wine = load_wine()\n","df = pd.DataFrame(wine.data, columns=wine.feature_names)\n","df['variety'] = wine.target\n","\n","# 対象変数の取り出し\n","cols = ['color_intensity', 'malic_acid']\n","X = df[cols]\n","\n","# 元の分布の表示\n","X.hist(figsize=(12, 4))\n","\n","# 対数変換\n","X_log = np.log(X)\n","cols_log = [s + '_log' for s in cols]\n","X_log.columns = cols_log\n","\n","# 対数変換後の分布の表示\n","X_log.hist(figsize=(12, 4))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FtqbeNEIGAMV"},"source":["対数変換により、それぞれの分布の歪みが補正されていることがわかります。\n","\n","これらを説明変数として、ロジスティック回帰の性能を比較してみましょう。\n","- 説明変数: `color_intensity`, `malic_acid`\n","- 目的変数: `variety` (ワインの種類, 3クラス)\n","- モデル: ロジスティック回帰\n","- 変数変換: 対数変換の有無\n","- 評価指標: F1スコア\n","- 検証方法: 5分割交差検証法"]},{"cell_type":"code","metadata":{"id":"wMi_CMMYGAMV"},"source":["# ライブラリの読み込み\n","from sklearn.model_selection import cross_val_score\n","\n","# 目的変数の用意\n","y = df['variety']\n","\n","# モデルの作成\n","mod = LogisticRegression(random_state=42)\n","\n","# モデルの学習と評価\n","score_wo_log = cross_val_score(mod, X, y, cv=5, scoring='f1_weighted').mean()\n","score_w_log = cross_val_score(mod, X_log, y, cv=5, scoring='f1_weighted').mean()\n","\n","print('CV score w/o log: {:.3f}'.format(score_wo_log))\n","print('CV score w/ log: {:.3f}'.format(score_w_log))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IPVb6nYtGAMV"},"source":["分布の歪んだ変数の対数変換により、ロジスティック回帰の性能が向上しているのがわかります。\n","\n","スケーリングで見たのと同様に、非決定木系のモデルは適切な非線形変換で性能を向上させます。"]},{"cell_type":"markdown","metadata":{"id":"PZ4jsohvGAMV"},"source":["**<練習問題 3>**  \n","今までに開催されたコンペティションのデータを用いて、数値変数の非線形変換の効果を検証してみましょう。"]},{"cell_type":"markdown","metadata":{"id":"mP54uvvjGAMV"},"source":["### 9.2.3 交差項の作成\n","数値変数に対する主要な特徴量エンジニアリング手法として、交差項の作成も挙げられます。  \n","交差項とは、複数の変数を掛け合わせて新たに作成される変数を指します。   \n","　例: \n","\\begin{equation}\n","(x_1, x_2, x_3) \\rightarrow (x_1x_2, x_2x_3, x_3x_1)\n","\\end{equation}\n","\n","交差項を作成する際のデメリットは、変数が増えることで計算コストが大きくなることです。  \n","そのため、後述の特徴選択などの変数を絞る手法と組み合わせて用いられることが多いです。\n","\n","ここではボストン住宅価格データセットの全ての変数を使って、交差項を作成してみます。"]},{"cell_type":"code","metadata":{"id":"v7Rycq1dGAMV"},"source":["# ライブラリの読み込み\n","from sklearn.datasets import load_boston\n","\n","# データの読み込み\n","boston = load_boston()\n","df = pd.DataFrame(boston.data, columns=boston.feature_names)\n","df['target'] = boston.target\n","\n","# データの確認\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CO_DuDkTGAMV"},"source":["説明変数は13種類の数値変数から構成されていることがわかります。  \n","これらから交差項を作成すると、${}_{13}\\mathrm{C}_2 = 91$ 種類の変数が増えます。   \n","このことを確認してみます。"]},{"cell_type":"code","metadata":{"id":"-fMImLz_GAMV"},"source":["# ライブラリの読み込み\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","# 説明変数の取り出し\n","X = df.iloc[:, :-1]\n","\n","# 交差項作成器の作成\n","pf = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n","\n","# 交差項の作成\n","X_pf = pf.fit_transform(X)\n","\n","# 結果の確認\n","print(\"変数の数: {}\".format(X_pf.shape[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HIQGGaS0GAMV"},"source":["**<練習問題4>**  \n","今までに開催されたコンペティションのデータを用いて、交差項を作成してみましょう。"]},{"cell_type":"markdown","metadata":{"id":"Vz_71XPnGAMV"},"source":["## 9.3 カテゴリ変数のエンコーディング\n","初めに述べたように、カテゴリカル変数は予め数値変数に変換しておく必要がありました。  \n","これをカテゴリカル変数のエンコーディングと言い、次のように様々な手法が存在します;\n","- Label Encoding\n","- Count Encoding\n","- Label-Count Encoding\n","- One-Hot Encoding（これらの他にも様々あります）\n","\n","Titanicデータセットのカテゴリカル変数 `embarked`に対して、それぞれの適用方法を見てみましょう。\n","- `embarked`: 乗船した港、値は `C`, `Q`, `S` の三種類"]},{"cell_type":"code","metadata":{"id":"R3pUD3esPttb"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cw1VOHs9GAMV"},"source":["# データの読み込み（pathにはコンペティション1の対応するパスを入れてください）\n","path = \"/content/drive/MyDrive/GCI_2022_Summer/Competitions/competition_1/\"\n","df = pd.read_csv(path + 'data/train.csv')\n","\n","# 対象変数の取り出し\n","x = df['Embarked']\n","\n","# 欠損値補完（最多の'S'で補完します）\n","x.fillna('S', inplace=True)\n","\n","# 対象変数の確認\n","x.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"68YbuTkAGAMV"},"source":["### 9.3.1 Label Encoding\n","Label Encodingでは、与えられたカテゴリカル変数に単純に数字を割り当てることでエンコーディングします。  \n","　例: `C` → 0, `Q` → 1, `S` → 2  \n","\n","この手法の問題点は、3種類以上の値を含むカテゴリカル変数に対して、無意味な順序が導入されてしまうことです。  \n","（例: CとQの差よりCとSの差の方が大きいということはないが、数値的にはそのようなことが含意されてしまう）"]},{"cell_type":"code","metadata":{"id":"kNAiw-45GAMV"},"source":["# ライブラリの読み込み\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Label Encoding器の作成\n","le = LabelEncoder()\n","\n","# Label Encodingの適用\n","x_le = le.fit_transform(x)\n","\n","# 適用結果の確認（最初の10件）\n","x_le[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2PhGJq5JGAMV"},"source":["### 9.3.2 Count Encoding\n","これに対してCount Encodingでは、与えられたカテゴリカル変数にそれぞれの値の出現回数を割り当てます。  \n","　例: `C`, `Q`, `S`の出現回数がそれぞれ168, 77, 644であれば、`C` → 168, `Q` → 77, `S` → 644  \n","\n","この手法の問題点は、異なるカテゴリカル変数に同じ値が割り当てられ、情報が失われる場合があることです。  \n","（例: `C`, `Q`の出現回数がそれぞれ1であれば、`C` → 1, `Q` → 1となり、同じ値になってしまう）"]},{"cell_type":"code","metadata":{"id":"RYaXsD04GAMV"},"source":["# Count Encodingの適用\n","x_ce = x.map(x.value_counts())\n","\n","# 適用結果の確認（最初の10件）\n","x_ce[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OhRKftBVGAMV"},"source":["### 9.3.3 Label-Count Encoding\n","Label-Count Encodingはその名の通り Label Encoding と Count Encoding を組み合わせたものです。  \n","Label-Count Encodingでは、与えられたカテゴリカル変数にそれぞれの値の出現回数の順位を割り当てます。  \n","　例: `C`, `Q`, `S`の出現回数がそれぞれ168(2位), 77(3位), 644(1位)であれば、`C` → 2, `Q` → 3, `S` → 1\n","\n","Label-Count Encodingは、無意味な順序付けというLabel Encodingの問題点を解消する手法となっています。  \n","\n","また出現回数が同じ場合に順位を登場順に決めることにすれば（rankメソッドの引数をmethod='first'とする）  \n","異なるカテゴリカル変数に対する値の衝突というCount Encodingの問題点も解消することができています。  \n","（ただしこの場合、出現回数が同じカテゴリカル変数だけに対しては、無意味な順序が導入されてしまいます）"]},{"cell_type":"code","metadata":{"id":"L8OVlLKLGAMV"},"source":["# Label-Count Encodingの適用\n","x_lce = x.map(x.value_counts().rank(ascending=False, method='first'))\n","\n","# 適用結果の確認（最初の10件）\n","x_lce[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LIv4yLz-GAMW"},"source":["### 9.3.4 One-Hot Encoding\n","One-Hot Encodingは、カテゴリカル変数のそれぞれの値に対応するカラムを用意して、フラグを割り当てます。  \n","この手法は、Label EncodingやCount Encodingで挙げられた問題点をいずれも解消することができています。\n","\n","例: \n","\n","|　index　|　Embarked　|　|→|　|　index　|　S　|　C　|　Q　|\n","|:-:|:-:|---|---|---|:-:|:-:|:-:|:-:|\n","|1 |S ||||1|1|0|0|\n","|2 |C ||||2|0|1|0|\n","|3 |S ||||3|1|0|0|\n","|4 |S ||||4|1|0|0|\n","|5 |S ||||5|1|0|0|\n","|6 |Q ||||6|0|0|1|\n","\n","One-Hot Encodingの問題点は、カテゴリカル変数の値の種類だけカラム数が増え、疎になってしまうことです。  \n","例えば1000種類の値をもつ変数に対してOne-Hot Encodingを適用すると、カラムが1000列増えてしまいます。  \n","このような場合は、前述のLabel-Count Encodingなど他の手法を適用した方が良いということになるでしょう。"]},{"cell_type":"code","metadata":{"id":"FHPp0jFFGAMW"},"source":["# One-Hot Encodingの適用\n","x_ohe = pd.get_dummies(x)\n","\n","# 適用結果の確認（最初の10件）\n","x_ohe[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xa9p324XGAMW"},"source":["**<練習問題5>**  \n","今までに開催されたコンペティションのデータを用いて、様々なエンコーディングを適用してみましょう。"]},{"cell_type":"markdown","metadata":{"id":"fWAjhHSTQzvt"},"source":["## 9.4 時間変数のエンコーディング\n","時間や日付のように数値でも周期性をもつ変数は、その構造を反映した形でエンコードする必要があります。  \n","ここでは日付を例にとって、時間変数のエンコーディング手法を見ていきましょう。\n","\n","まずここで扱うサンプルデータとして、2010年代の日付をランダムに10個取得します。"]},{"cell_type":"code","metadata":{"id":"6qD-92M_fBNy"},"source":["import datetime\n","import random\n","\n","# startとendの間のランダムな日付を出力する関数\n","def generate_rand_date(start, end):\n","    days_delta = (end - start).days\n","    return start + datetime.timedelta(days=random.randint(0, days_delta))\n","\n","# 乱数シードを固定して、2010年代の日付をランダムに10個取得\n","random.seed(0)\n","start = datetime.datetime(2010, 1, 1)\n","end = datetime.datetime(2020, 12, 31)\n","random_dates = [generate_rand_date(start, end) for _ in range(10)]\n","\n","# 日付データをDataFrame化\n","df_dates = DataFrame(random_dates, columns=['ymd'])\n","df_dates"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ECGnN0vgjYHq"},"source":["### 9.4.1 周期性を考慮しないエンコーディング\n","まずは、周期性を考慮しないエンコーディング手法をいくつか見てみましょう。\n","\n","最も単純なエンコーディング手法は年月日それぞれを数値変数とする方法です。\n"]},{"cell_type":"code","metadata":{"id":"F4htZNiCoQgt"},"source":["# 年月日を、年・月・日に分割\n","df_dates['year'] = df_dates['ymd'].apply(lambda x: x.year)\n","df_dates['month'] = df_dates['ymd'].apply(lambda x: x.month)\n","df_dates['day'] = df_dates['ymd'].apply(lambda x: x.day)\n","\n","df_dates"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k7PP_WXDrVli"},"source":["このようにすることで、年月日それぞれを数値変数として扱うことができるようになりました。  \n","しかしこのような手法に対しては以下のように、大きく二つの問題点を挙げることができます。\n","1. 年・月・日の情報がバラバラであり時系列的な新旧の学習が困難  \n","(例: 2010年1月1日と2020年1月1日が時系列的に離れているということを表現できていない)\n","2. 時系列として連続的であるべき部分に断絶が生じてしまっている  \n","(例: 1月と12月、1日と31日がそれぞれ周期的な意味で近いということを表現できていない)\n","\n","前者に対処するためには、基準時点との差分を新たな変数とする手法を用いることができます。  \n","このようにエンコードすることで、日付の新旧という情報を表すことができるようになります。  \n","今回は日付を扱っているので2010年1月1日を基準日とし、差分を新たな変数としてみましょう。"]},{"cell_type":"code","metadata":{"id":"WAF6rQgelTKI"},"source":["# 基準日を2010年1月1日に設定\n","start_date = pd.Timestamp('2010-01-01 00:00:00')\n","\n","# 基準日との差分を格納\n","df_dates['total_days'] = df_dates['ymd'].apply(lambda x: (x - start_date).days)\n","\n","df_dates"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NHDQ5G3QuYqW"},"source":["このようにすることで、時系列的な新旧の情報を明示的にエンコードすることができました。  \n","次に周期性を考慮したエンコーディング手法も用いることで、後者の問題に対処しましょう。"]},{"cell_type":"markdown","metadata":{"id":"3PGngqEPme7G"},"source":["### 9.4.2 周期性を考慮したエンコーディング\n","時間変数の周期的な構造をエンコードするためには、三角関数を用いることができます。  \n","月や日といった周期性をもつものを単位円周上の点に対応づけて、各点の座標で表します。  \n","このとき各点の座標は$\\cosと\\sin$で表されるので、これらを新たな数値変数として用います。"]},{"cell_type":"code","metadata":{"id":"e97AaXI5xp-Y"},"source":["df_dates['month_cos'] = df_dates['month'].apply(lambda x: np.cos(2*np.pi * x/12))\n","df_dates['month_sin'] = df_dates['month'].apply(lambda x: np.sin(2*np.pi * x/12))\n","\n","df_dates['day_cos'] = df_dates['day'].apply(lambda x: np.cos(2*np.pi * x/31))\n","df_dates['day_sin'] = df_dates['day'].apply(lambda x: np.sin(2*np.pi * x/31))\n","\n","df_dates"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Pe3hGbkyUub"},"source":["このようにすることで、周期的な構造を踏まえて時間変数をエンコードすることができました。"]},{"cell_type":"markdown","metadata":{"id":"d5xFoGd4GAMW"},"source":["## 9.5 次元削減と特徴選択\n","元々の説明変数が多かったり、特徴量エンジニアリングで変数が増えたりすると、計算コストが大きくなります。  \n","このように計算コストが問題となったりモデルの説明性が求められるような場合、変数を減らす必要があります。  \n","変数を減らす手法には、次元削減と特徴選択があります。ここでは、そのそれぞれの代表的な手法を紹介します。"]},{"cell_type":"markdown","metadata":{"id":"oglWlQhyGAMW"},"source":["### 9.5.1 次元削減\n","教師なし学習の章で学んだPCA（主成分分析）は、説明変数の次元削減に応用することができます。  \n","PCAを用いた次元削減は、次のように実行できます;  \n","\n","1. 変数をスケーリングする\n","2. PCAを施す\n","3. 寄与率などを参考に変換後の変数を絞る\n","\n","ここでは乳がんデータセットを用い、PCAによりデータの次元を元の30次元から削減してみます。  \n","ただしPCAを適用した後の変数は、累積寄与率がおおよそ80%となるように次元削減を行います。"]},{"cell_type":"code","metadata":{"id":"d-ASgWgZGAMW"},"source":["# ライブラリの読み込み\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.decomposition import PCA\n","\n","# データの読み込み\n","cancer = load_breast_cancer()\n","df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n","df['target'] = cancer.target\n","\n","# 説明変数の取り出し\n","X = df.iloc[:, :-1]\n","\n","# スケーリング\n","ss = StandardScaler()\n","X_std = ss.fit_transform(X)\n","\n","# PCAの適用\n","pca = PCA()\n","X_pca = pca.fit_transform(X_std)\n","\n","# 累積寄与率の確認\n","ev_ratio = pca.explained_variance_ratio_\n","ev_ratio = ev_ratio.cumsum().round(3)\n","ev_ratio"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xrTmPKijGAMW"},"source":["この結果から第4主成分までを採用すれば、累積寄与率がおよそ0.8となることがわかります。  \n","つまりこの累積寄与率0.8の基準であれば、次元を元の30次元から4次元まで削減できました。\n","\n","ついでに、第1主成分と第2主成分を用いてデータを二次元平面上にプロットしてみましょう。"]},{"cell_type":"code","metadata":{"id":"gAHnLNsPGAMW"},"source":["# 目的変数の取り出し\n","y = df.iloc[:, -1]\n","\n","# 散布図を表示\n","color_list = list(map(lambda x: ['b', 'r'][x], y))\n","plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.3, color=color_list)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sYf7BJhUGAMW"},"source":["**<練習問題6>**  \n","元のデータで学習させたモデルの性能と、PCAで次元削減したモデルの性能を比較してみましょう。"]},{"cell_type":"markdown","metadata":{"id":"JansfwdBGAMW"},"source":["**<練習問題7>**  \n","今までに開催されたコンペティションのデータに対して、PCAによる次元削減を行ってみましょう。"]},{"cell_type":"markdown","metadata":{"id":"UBxQ0EkyGAMW"},"source":["### 9.5.2 特徴選択\n","PCAによる次元削減では変数を主成分に変換した上で、寄与率に基づいて絞り込みました。  \n","一方、特徴選択では、変数をそのままに、回帰係数や変数重要度に基づいて絞り込みます。  \n","そのため、特徴選択では回帰係数や変数重要度を出力できるモデルを与える必要があります。\n","- 回帰: ランダムフォレスト回帰、リッジ回帰 etc.\n","- 分類: ランダムフォレスト分類、ロジスティック回帰 etc.\n","\n","ここでは2.3の交差項の作成で大量に作成されてしまった変数を10個に絞り込みましょう。  \n","設定するモデルは、変数重要度の出力が可能であるランダムフォレスト回帰モデルとします。"]},{"cell_type":"code","metadata":{"id":"K04wvnwaGAMW"},"source":["# ライブラリの読み込み\n","from sklearn.feature_selection import RFE\n","from sklearn.ensemble import RandomForestRegressor\n","\n","# データの読み込み\n","boston = load_boston()\n","df = pd.DataFrame(boston.data, columns=boston.feature_names)\n","df['target'] = boston.target\n","\n","# 説明変数と目的変数の用意\n","X = df.iloc[:, :-1]\n","y = df.iloc[:, -1]\n","\n","# 交差項作成器の作成\n","pf = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n","\n","# 交差項の作成\n","X_pf = pf.fit_transform(X)\n","\n","# 説明変数の結合\n","X_all = X.join(DataFrame(X_pf))\n","\n","# 特徴選択器の作成\n","rfr = RandomForestRegressor(random_state=42)\n","rfe = RFE(estimator=rfr, n_features_to_select=10)\n","\n","# 特徴選択の実行\n","rfe.fit(X_all, y)\n","\n","# 結果の確認\n","X_rfe = X_all.loc[:, rfe.support_]\n","X_rfe.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x7ZTuoALGAMW"},"source":["このようにして、104種類あった変数を10種類まで選択することができました。"]},{"cell_type":"markdown","metadata":{"id":"dn9M79OfGAMW"},"source":["**<練習問題8>**  \n","元データ・交差項追加データ・特徴選択データそれぞれで、モデルの性能と学習時間を比較してみましょう。"]},{"cell_type":"markdown","metadata":{"id":"dZS1znNgGAMW"},"source":["**<練習問題9>**  \n","今までに開催されたコンペティションのデータに対して、特徴選択を行ってみましょう。"]},{"cell_type":"code","metadata":{"id":"OlNixileGAMW"},"source":[""],"execution_count":null,"outputs":[]}]}